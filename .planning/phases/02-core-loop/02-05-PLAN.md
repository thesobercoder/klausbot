---
phase: 02-core-loop
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - src/memory/embeddings.ts
  - src/memory/search.ts
  - src/memory/logger.ts
  - src/memory/context.ts
  - src/memory/index.ts
  - package.json
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Semantic search returns relevant memories when queried about past topics"
    - "User can query past conversations using natural language (not just keywords)"
  artifacts:
    - path: "src/memory/embeddings.ts"
      provides: "Embedding generation and storage"
      exports: ["generateEmbedding", "storeEmbedding", "initializeEmbeddings"]
    - path: "src/memory/search.ts"
      provides: "Semantic search over stored embeddings"
      exports: ["semanticSearch"]
  key_links:
    - from: "src/memory/logger.ts"
      to: "src/memory/embeddings.ts"
      via: "storeEmbedding call after logging"
      pattern: "storeEmbedding"
    - from: "src/memory/context.ts"
      to: "retrieval instructions"
      via: "semantic search instructions added"
      pattern: "semantic.*search"
---

<objective>
Close MEM-05 gap: Add vector embedding infrastructure for semantic memory retrieval.

Purpose: Enable Claude to find relevant past conversations using semantic similarity, not just keyword matching. User query "what did we discuss about my family?" should find conversations mentioning relatives, parents, siblings - even if "family" was never used.

Output:
- embeddings.ts: Generate + store OpenAI text-embedding-3-small vectors
- search.ts: Cosine similarity search over stored embeddings
- Modified logger.ts: Auto-embed after logging
- Modified context.ts: Instructions for semantic search
</objective>

<execution_context>
@/home/soham/.claude/get-shit-done/workflows/execute-plan.md
@/home/soham/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-core-loop/02-VERIFICATION.md
@src/memory/logger.ts
@src/memory/context.ts
@src/memory/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add embedding infrastructure</name>
  <files>src/memory/embeddings.ts, package.json</files>
  <action>
Create embeddings.ts with:

1. **Dependencies:** Add `openai` package to package.json (latest v4.x)

2. **Embedding generation:**
   - Use OpenAI text-embedding-3-small (1536 dimensions, $0.00002/1K tokens)
   - Function: `generateEmbedding(text: string): Promise<number[]>`
   - Use OPENAI_API_KEY from env (lazy initialization like config pattern)

3. **Embedding storage:**
   - Store in ~/.klausbot/embeddings.json (simple JSON file)
   - Format: `{ entries: [{ id, text, embedding, timestamp, source }] }`
   - Function: `storeEmbedding(text: string, source: string): Promise<void>`
   - Chunking: Split long texts at ~500 chars for better retrieval

4. **Initialization:**
   - Function: `initializeEmbeddings(): void` - creates file if missing
   - Handle missing OPENAI_API_KEY gracefully (log warning, skip embedding)

5. **Error handling:**
   - Rate limit: Log and skip (don't fail the message flow)
   - API error: Log and skip (degraded mode - grep still works)

DO NOT block the main message flow on embedding failures. This is an enhancement, not critical path.
  </action>
  <verify>
    - npm install succeeds
    - npm run build succeeds
    - embeddings.ts exports generateEmbedding, storeEmbedding, initializeEmbeddings
  </verify>
  <done>
    Embedding module exists with OpenAI integration, graceful degradation on errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Create semantic search module</name>
  <files>src/memory/search.ts</files>
  <action>
Create search.ts with:

1. **Cosine similarity function:**
   - Pure TypeScript (no dependencies)
   - Function: `cosineSimilarity(a: number[], b: number[]): number`

2. **Semantic search function:**
   - Function: `semanticSearch(query: string, topK?: number): Promise<SearchResult[]>`
   - SearchResult: `{ text: string, score: number, source: string, timestamp: string }`
   - Default topK = 5
   - Load embeddings.json, generate query embedding, find top K by similarity
   - Filter by minimum score threshold (0.7)

3. **Search output format:**
   - Return markdown-friendly format for Claude to consume
   - Include source reference (which conversation file)
   - Sort by relevance score descending

4. **Handle edge cases:**
   - No embeddings file: return empty array
   - No OPENAI_API_KEY: return empty array with warning
   - All scores below threshold: return empty array
  </action>
  <verify>
    - npm run build succeeds
    - search.ts exports semanticSearch
    - cosineSimilarity(a, a) returns 1.0 for same vector
  </verify>
  <done>
    Semantic search module returns relevant results sorted by score
  </done>
</task>

<task type="auto">
  <name>Task 3: Wire embedding to logger and update instructions</name>
  <files>src/memory/logger.ts, src/memory/context.ts, src/memory/index.ts</files>
  <action>
1. **Modify logger.ts:**
   - Import storeEmbedding from ./embeddings.js
   - After logAssistantMessage writes to file, call:
     `storeEmbedding(content, 'assistant-' + getToday()).catch(() => {})`
   - Fire-and-forget (.catch to avoid blocking)
   - Only embed assistant messages (user messages less useful for retrieval)

2. **Modify context.ts getRetrievalInstructions():**
   - Add semantic search section to memory instructions:
   ```
   ## Semantic Search

   For conceptual queries ("what did we discuss about X?"), use semantic search:
   1. Read ~/.klausbot/embeddings.json
   2. The search.ts module provides semanticSearch() for similarity matching
   3. Semantic search finds related content even without exact keywords

   Fallback to Grep if semantic search unavailable or OPENAI_API_KEY not set.
   ```

3. **Update index.ts exports:**
   - Add exports from ./embeddings.js and ./search.js
   - Export: initializeEmbeddings, storeEmbedding, semanticSearch

4. **Gateway initialization:**
   - In gateway.ts (if needed), call initializeEmbeddings() after initializeHome()
  </action>
  <verify>
    - npm run build succeeds
    - Logger calls storeEmbedding after logAssistantMessage
    - Context instructions mention semantic search
    - Index.ts exports all new functions
  </verify>
  <done>
    Embedding generation wired to message flow, instructions updated for Claude
  </done>
</task>

</tasks>

<verification>
After all tasks:

1. **Build verification:**
   ```bash
   npm run build
   ```

2. **Dependency check:**
   ```bash
   npm ls openai
   ```

3. **Export verification:**
   ```bash
   grep -r "export.*semanticSearch\|storeEmbedding\|initializeEmbeddings" src/memory/
   ```

4. **Wiring verification:**
   - logger.ts imports from embeddings.ts
   - context.ts mentions semantic search in instructions
   - index.ts re-exports new functions

5. **Graceful degradation:**
   - Code handles missing OPENAI_API_KEY without crashing
</verification>

<success_criteria>
1. Build succeeds with new openai dependency
2. embeddings.ts generates and stores embeddings via OpenAI API
3. search.ts performs cosine similarity search over stored embeddings
4. logger.ts auto-embeds assistant responses (fire-and-forget)
5. context.ts includes semantic search in Claude's retrieval instructions
6. Missing OPENAI_API_KEY degrades gracefully (grep fallback)
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-loop/02-05-SUMMARY.md`
</output>
