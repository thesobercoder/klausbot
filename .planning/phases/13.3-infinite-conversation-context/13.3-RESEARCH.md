# Phase 13.3: Infinite Conversation Context - Research

**Researched:** 2026-02-06
**Domain:** Conversation history injection, thread detection, tiered context budgeting, retrieval-first instruction design
**Confidence:** HIGH

## Summary

Phase 13.3 transforms klausbot from a stateless-per-session bot into one that carries full conversation context across Claude CLI invocations. Currently, each Claude session receives only 3 recent conversation summaries via the SessionStart hook (see `src/cli/hook.ts`). The user's actual conversation thread -- what was said, when, in what order -- is lost. Claude must use MCP tools to search history reactively, which it often fails to do unprompted.

The solution injects conversation history directly into the system prompt (via `--append-system-prompt`) as structured XML blocks with timestamps and relative time labels. A tiered strategy keeps recent conversations verbatim while summarizing older ones, all within a ~30K token budget. Thread detection uses time proximity to determine whether the current message continues an existing conversation or starts a new one.

**Primary recommendation:** Build a `buildConversationContext()` function in `src/memory/context.ts` that queries SQLite conversations by chatId, applies tiered formatting (full transcript for recent, summaries for older), estimates token count via character ratio, and returns XML-wrapped history. Inject this into `buildSystemPrompt()` and rewrite retrieval instructions to enforce search-first behavior.

## Current State Analysis

### What Exists

| Component               | Location                                           | Current Behavior                                                             | Gap                                                                 |
| ----------------------- | -------------------------------------------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| SessionStart hook       | `src/cli/hook.ts`                                  | Injects datetime + 3 recent summaries (date + 1-liner)                       | No full transcripts, no timestamps, no thread detection             |
| System prompt           | `src/memory/context.ts:buildSystemPrompt()`        | Identity + retrieval instructions + skill/agent reminders                    | No conversation history                                             |
| Retrieval instructions  | `src/memory/context.ts:getRetrievalInstructions()` | Tells Claude to use `search_memories` MCP tool                               | Passive ("when in doubt, search") not assertive ("you MUST search") |
| Conversation storage    | `src/memory/conversations.ts`                      | SQLite table with sessionId, transcript (JSONL), summary, chatId, timestamps | Already has everything needed for retrieval                         |
| MCP tools               | `src/mcp-server/tools/`                            | `search_memories` (FTS5 + semantic), `get_conversation` (full transcript)    | Tools exist but Claude doesn't proactively use them                 |
| Gateway prompt building | `src/daemon/gateway.ts:processMessage()`           | Builds `additionalInstructions` with chatId + notes + orchestration          | Good injection point for conversation context                       |

### Data Flow for Context Injection

```
User message arrives (gateway.ts)
  → processMessage() builds additionalInstructions
  → queryClaudeCode() or streamClaudeResponse() called
    → buildSystemPrompt() creates base prompt (identity + instructions)
    → additionalInstructions appended to systemPrompt
    → Passed via --append-system-prompt to Claude CLI
    → SessionStart hook ALSO injects context via stdout
```

**Key insight:** There are TWO injection points:

1. `--append-system-prompt` (synchronous, built in gateway before spawn)
2. SessionStart hook stdout (async, runs after spawn)

For conversation history, use `--append-system-prompt` (injection point 1) because:

- It's synchronous: no race conditions
- Built in gateway where we have chatId directly
- SessionStart hook is for lightweight metadata (datetime, session ID)
- History needs chatId which is available in gateway but must be passed via env to hooks

### Token Budget Analysis

Current system prompt components (estimated):

- Skill reminder: ~50 tokens
- Agent reminder: ~100 tokens
- Identity files (SOUL.md + IDENTITY.md + USER.md + REMINDERS.md): ~500-2000 tokens
- Retrieval instructions: ~1500 tokens
- Orchestration instructions: ~800 tokens
- Session context (chatId + notes): ~100 tokens

**Total current:** ~3,000-4,500 tokens

Claude's context window: 200K tokens
Target history budget: 30K tokens
Remaining for conversation: ~165K tokens

**30K tokens ~ 120,000 characters** (using 4:1 char-to-token ratio)

This is generous. A typical Telegram conversation turn is 50-200 chars user + 200-1000 chars assistant. At ~600 chars/turn average, 30K tokens supports ~200 full conversation turns, or many more with tiered summarization.

## Standard Stack

### Core (Already Installed)

| Library        | Version | Purpose                   | Already In Project |
| -------------- | ------- | ------------------------- | ------------------ |
| drizzle-orm    | 0.45.1  | Query conversations table | Yes                |
| better-sqlite3 | 12.6.2  | SQLite driver             | Yes                |

### No New Dependencies Required

This phase requires zero new npm packages. All functionality is built on:

- Existing SQLite conversation storage
- Existing Drizzle ORM queries
- String manipulation for XML formatting
- Character-based token estimation

### Token Estimation Approach

**Use:** Character-based estimation at 4:1 ratio (4 chars per token)

**Why not a tokenizer library:**

- Anthropic has no official local tokenizer for Claude 3+ models
- The only official method is the Token Count API (requires API call per count)
- `js-tiktoken` with `p50k_base` is only an approximation anyway
- Character-based estimation is sufficient for budget enforcement -- we're not billing, just capping

**Implementation:**

```typescript
function estimateTokens(text: string): number {
  return Math.ceil(text.length / 4);
}
```

Confidence: HIGH -- 4:1 ratio is the standard approximation per Anthropic docs and community consensus.

## Architecture Patterns

### Recommended File Changes

```
src/
  memory/
    context.ts           # MODIFY: Add buildConversationContext(), update buildSystemPrompt()
    conversations.ts     # MODIFY: Add getConversationsForContext() query method
  cli/
    hook.ts              # MODIFY: Simplify SessionStart (history moves to system prompt)
  daemon/
    gateway.ts           # MODIFY: Pass chatId to buildSystemPrompt()
    spawner.ts           # MINOR: Thread chatId through to buildSystemPrompt()
  telegram/
    streaming.ts         # MINOR: Same chatId threading
```

### Pattern 1: Thread Detection via Time Proximity

**What:** Determine if current message is a continuation of recent conversation or a new thread.
**When to use:** Every message processing, before building context.

```typescript
// Thresholds for conversation continuity
const ACTIVE_THREAD_WINDOW_MS = 30 * 60 * 1000; // 30 minutes
const RECENT_WINDOW_MS = 4 * 60 * 60 * 1000; // 4 hours
const TODAY_WINDOW_MS = 24 * 60 * 60 * 1000; // 24 hours

interface ThreadDetection {
  /** Is this a continuation of an active conversation? */
  isContinuation: boolean;
  /** The active thread's conversations (if continuation) */
  activeThread: ConversationRecord[];
  /** Label: "continuation" | "new" */
  threadType: "continuation" | "new";
}

function detectThread(chatId: number): ThreadDetection {
  const recent = getRecentConversations(1, undefined, chatId);

  if (recent.length === 0) {
    return { isContinuation: false, activeThread: [], threadType: "new" };
  }

  const lastEnded = new Date(recent[0].endedAt).getTime();
  const elapsed = Date.now() - lastEnded;

  if (elapsed < ACTIVE_THREAD_WINDOW_MS) {
    // Within 30 min: active continuation
    // Fetch all conversations in this thread cluster
    const threadConversations = getConversationsInWindow(
      chatId,
      ACTIVE_THREAD_WINDOW_MS,
    );
    return {
      isContinuation: true,
      activeThread: threadConversations,
      threadType: "continuation",
    };
  }

  return { isContinuation: false, activeThread: [], threadType: "new" };
}
```

### Pattern 2: Tiered Context Building

**What:** Full transcripts for recent, summaries for older, within token budget.
**When to use:** Building system prompt before each Claude invocation.

```
Tier 1 (FULL TRANSCRIPT): Active thread (last 30 min) -- highest priority
Tier 2 (FULL TRANSCRIPT): Today's other conversations -- if budget allows
Tier 3 (SUMMARY ONLY): Yesterday's conversations
Tier 4 (SUMMARY ONLY): Older conversations (up to 7 days)
```

```typescript
interface ConversationContextOptions {
  chatId: number;
  maxTokens?: number; // Default: 30000
}

function buildConversationContext(options: ConversationContextOptions): string {
  const { chatId, maxTokens = 30000 } = options;
  const maxChars = maxTokens * 4;

  let usedChars = 0;
  const sections: string[] = [];
  const now = Date.now();

  // Tier 1: Active thread (full transcripts)
  const activeThread = getConversationsInWindow(
    chatId,
    ACTIVE_THREAD_WINDOW_MS,
  );
  if (activeThread.length > 0) {
    for (const conv of activeThread) {
      const formatted = formatFullTranscript(conv);
      if (usedChars + formatted.length > maxChars) break;
      sections.push(formatted);
      usedChars += formatted.length;
    }
  }

  // Tier 2: Today's other conversations (full transcripts if budget allows)
  const todayConversations = getConversationsInWindow(
    chatId,
    TODAY_WINDOW_MS,
  ).filter((c) => !activeThread.some((a) => a.sessionId === c.sessionId));

  for (const conv of todayConversations) {
    const formatted = formatFullTranscript(conv);
    if (usedChars + formatted.length > maxChars) {
      // Fall back to summary
      const summary = formatSummary(conv, "today");
      if (usedChars + summary.length <= maxChars) {
        sections.push(summary);
        usedChars += summary.length;
      }
    } else {
      sections.push(formatted);
      usedChars += formatted.length;
    }
  }

  // Tier 3: Yesterday (summaries only)
  const yesterdayConvs = getConversationsInRange(chatId, 1, 2); // 1-2 days ago
  for (const conv of yesterdayConvs) {
    const summary = formatSummary(conv, "yesterday");
    if (usedChars + summary.length > maxChars) break;
    sections.push(summary);
    usedChars += summary.length;
  }

  // Tier 4: Older (summaries only, up to 7 days)
  const olderConvs = getConversationsInRange(chatId, 2, 7); // 2-7 days ago
  for (const conv of olderConvs) {
    const summary = formatSummary(conv, "older");
    if (usedChars + summary.length > maxChars) break;
    sections.push(summary);
    usedChars += summary.length;
  }

  if (sections.length === 0) return "";

  return sections.join("\n\n");
}
```

### Pattern 3: XML-Tagged History Injection

**What:** Format conversation history with XML tags, timestamps, and relative time labels.
**Why XML:** Consistent with existing codebase pattern (identity files use `<FILENAME>` tags, session context uses `<session-context>` tags).

```typescript
function formatFullTranscript(conv: ConversationRecord): string {
  const entries = parseTranscript(conv.transcript);
  const relativeTime = getRelativeTimeLabel(conv.endedAt);
  const timestamp = new Date(conv.startedAt).toISOString();

  const messages = entries
    .filter((e) => e.type === "user" || e.type === "assistant")
    .map((e) => {
      const role = e.type === "user" ? "human" : "you";
      const time = e.timestamp
        ? new Date(e.timestamp).toLocaleTimeString()
        : "";
      const text = extractText(e);
      return `[${role}${time ? " " + time : ""}] ${text}`;
    })
    .join("\n");

  return `<conversation timestamp="${timestamp}" relative="${relativeTime}">
${messages}
</conversation>`;
}

function formatSummary(conv: ConversationRecord, period: string): string {
  const timestamp = new Date(conv.startedAt).toISOString();
  return `<conversation-summary timestamp="${timestamp}" relative="${period}">
${conv.summary}
</conversation-summary>`;
}
```

### Pattern 4: Continuation Signal in System Prompt

**What:** Tell Claude explicitly whether this is a continuation or new conversation.
**Why:** Success criterion #5 requires Claude to know its thread state.

```typescript
function buildThreadContext(detection: ThreadDetection): string {
  if (detection.isContinuation) {
    return `<thread-status>CONTINUATION — You are in an ongoing conversation. The human just sent another message. Your recent conversation history is below. Do NOT greet them or reintroduce yourself.</thread-status>`;
  }
  return `<thread-status>NEW CONVERSATION — This is a new conversation or a return after a break. Recent history is provided for context but treat this as a fresh interaction.</thread-status>`;
}
```

### Pattern 5: Retrieval-First Instructions

**What:** Rewrite retrieval instructions to enforce "search first, never say I don't remember."
**Why:** Success criterion #4 requires assertive search-first behavior.

Key changes to `getRetrievalInstructions()`:

```
CURRENT (passive):
  "When in doubt, search."
  "Don't assume context is complete."

NEW (assertive):
  "BEFORE saying you don't know or don't remember: SEARCH."
  "If the user references ANYTHING from the past, you MUST call search_memories."
  "NEVER say 'I don't have context' or 'I don't remember'. You DO have memory. Search it."
  "Responding without searching when the user references history is a FAILURE."
```

### Anti-Patterns to Avoid

- **Injecting history via SessionStart hook:** Hook runs asynchronously after spawn. History should be in `--append-system-prompt` for guaranteed availability.
- **Fetching all conversations without chatId filter:** Would cross-contaminate multi-user memory. Always filter by chatId.
- **Counting tokens via API call:** Adds latency per message. Character estimation is sufficient.
- **Including tool-use entries in transcript injection:** Bloats history with tool calls. Extract text-only turns.
- **Fixed conversation count instead of time-based tiers:** "Last 5 conversations" might span minutes or weeks. Time-based windowing is more predictable.

## Don't Hand-Roll

| Problem                   | Don't Build                   | Use Instead                                                | Why                                                                                  |
| ------------------------- | ----------------------------- | ---------------------------------------------------------- | ------------------------------------------------------------------------------------ |
| Token counting            | Tokenizer library integration | `Math.ceil(text.length / 4)`                               | No official local tokenizer for Claude 3+; character ratio is standard approximation |
| Conversation storage      | New storage layer             | Existing `conversations` SQLite table                      | Already has transcript, summary, chatId, timestamps                                  |
| Transcript parsing        | New parser                    | Existing `parseTranscript()` / `extractConversationText()` | Already tested in `src/memory/conversations.ts`                                      |
| Summary generation        | New summarizer                | Existing summaries stored at SessionEnd                    | Already generated and stored via hook                                                |
| Context window management | Manual compaction             | Claude Code's auto-compaction                              | Claude handles its own context overflow                                              |

**Key insight:** The entire data layer already exists from Phase 7.2. This phase is about reading that data in a new way (time-windowed, tiered) and injecting it into the system prompt.

## Common Pitfalls

### Pitfall 1: System Prompt Size Explosion

**What goes wrong:** Injecting too much history makes the system prompt enormous, consuming budget for the actual conversation.
**Why it happens:** No budget enforcement; naive "inject everything" approach.
**How to avoid:** Hard character limit (120K chars = ~30K tokens). Build context incrementally, stop when budget exhausted.
**Warning signs:** Claude auto-compacting immediately, high per-message costs.

### Pitfall 2: Stale Thread Detection

**What goes wrong:** User sends message 31 minutes after last conversation; system treats as "new" when it's clearly a continuation.
**Why it happens:** Hard cutoff at 30 minutes.
**How to avoid:** Use 30-minute window as the "active thread" cutoff, but still inject today's conversations as Tier 2 context. Claude sees the history either way; only the greeting behavior changes.
**Warning signs:** Claude re-greets user mid-conversation.

### Pitfall 3: Missing chatId in Background Agent Path

**What goes wrong:** Background agents get wrong conversation history or no history.
**Why it happens:** Background agents use `--resume` and don't rebuild system prompt.
**How to avoid:** Background agents already have session context from the parent. No history injection needed for `--resume` path. Only the initial dispatch path needs history.
**Warning signs:** N/A -- background agents continue existing sessions.

### Pitfall 4: Empty History on First Message

**What goes wrong:** First-ever message has no history; code must handle gracefully.
**Why it happens:** New user, no conversations stored yet.
**How to avoid:** `buildConversationContext()` returns empty string when no conversations found. Calling code handles empty string by omitting the section entirely.
**Warning signs:** XML tags with empty content injected.

### Pitfall 5: Duplicate History from Hook + System Prompt

**What goes wrong:** SessionStart hook still injects 3 summaries AND system prompt injects full history.
**Why it happens:** Forgot to update SessionStart hook after moving history to system prompt.
**How to avoid:** Simplify SessionStart hook to only inject datetime + session ID. Remove the summary injection from hook since history is now in system prompt.
**Warning signs:** Claude sees duplicate conversation summaries.

### Pitfall 6: JSONL Transcript Bloat from Tool Use

**What goes wrong:** Full transcript includes hundreds of lines of tool-use JSON (file reads, writes, searches).
**Why it happens:** Claude's JSONL transcript includes every tool call and result.
**How to avoid:** When formatting full transcripts for injection, only extract `user` and `assistant` text content blocks. Skip tool_use, tool_result, and system entries.
**Warning signs:** A 5-turn conversation consuming 20K tokens due to embedded tool results.

## Code Examples

### Query: Get Conversations in Time Window

```typescript
// New function in src/memory/conversations.ts
export function getConversationsInWindow(
  chatId: number,
  windowMs: number,
  limit: number = 20,
): ConversationRecord[] {
  const cutoff = new Date(Date.now() - windowMs).toISOString();
  const db = getDrizzle();

  return db
    .select()
    .from(conversations)
    .where(
      and(eq(conversations.chatId, chatId), gte(conversations.endedAt, cutoff)),
    )
    .orderBy(desc(conversations.endedAt))
    .limit(limit)
    .all() as ConversationRecord[];
}
```

### Query: Get Conversations in Date Range

```typescript
// For "yesterday" and "older" tiers
export function getConversationsInRange(
  chatId: number,
  daysAgoStart: number,
  daysAgoEnd: number,
  limit: number = 10,
): ConversationRecord[] {
  const start = new Date();
  start.setDate(start.getDate() - daysAgoEnd);
  const end = new Date();
  end.setDate(end.getDate() - daysAgoStart);

  const db = getDrizzle();

  return db
    .select()
    .from(conversations)
    .where(
      and(
        eq(conversations.chatId, chatId),
        gte(conversations.endedAt, start.toISOString()),
        lte(conversations.endedAt, end.toISOString()),
      ),
    )
    .orderBy(desc(conversations.endedAt))
    .limit(limit)
    .all() as ConversationRecord[];
}
```

### Integration: Updated buildSystemPrompt()

```typescript
// Signature change: now accepts chatId
export function buildSystemPrompt(chatId?: number): string {
  // Bootstrap mode unchanged
  const bootstrapPath = getHomePath("identity", "BOOTSTRAP.md");
  if (existsSync(bootstrapPath)) {
    return readFileSync(bootstrapPath, "utf-8");
  }

  const skillReminder = getSkillReminder();
  const agentReminder = getAgentReminder();
  const identity = loadIdentity();
  const instructions = getRetrievalInstructions();

  let prompt =
    skillReminder +
    "\n\n" +
    agentReminder +
    "\n\n" +
    identity +
    "\n\n" +
    instructions;

  // Inject conversation history if chatId available
  if (chatId !== undefined) {
    const history = buildConversationContext({ chatId });
    if (history) {
      prompt += "\n\n" + history;
    }
  }

  return prompt;
}
```

### Integration: Gateway chatId Threading

```typescript
// In gateway.ts processMessage(), both streaming and batch paths already have msg.chatId
// The system prompt is built inside queryClaudeCode() and streamClaudeResponse()
// Need to thread chatId into buildSystemPrompt()

// Option A: Pass chatId through SpawnerOptions (already has chatId field!)
// spawner.ts already receives options.chatId -- just forward to buildSystemPrompt()

// In spawner.ts queryClaudeCode():
let systemPrompt = buildSystemPrompt(options.chatId);

// In streaming.ts streamClaudeResponse():
let systemPrompt = buildSystemPrompt(options.chatId);
```

## State of the Art

| Old Approach                      | Current Approach                                         | When Changed | Impact                                           |
| --------------------------------- | -------------------------------------------------------- | ------------ | ------------------------------------------------ |
| 3 summaries via SessionStart hook | Full tiered history via system prompt                    | Phase 13.3   | Claude has actual conversation context           |
| Passive "when in doubt, search"   | Assertive "you MUST search before saying you don't know" | Phase 13.3   | Eliminates "I don't have that context" responses |
| No thread detection               | Time-proximity thread detection                          | Phase 13.3   | Claude knows continuation vs. new                |
| History only via MCP tool calls   | Pre-injected + MCP for deep retrieval                    | Phase 13.3   | Immediate context without tool call latency      |

## Open Questions

1. **30K token budget: right size?**
   - What we know: Current system prompt is ~3-5K tokens. Claude has 200K context.
   - What's unclear: How much budget should remain for the actual conversation + tool use?
   - Recommendation: 30K is conservative and safe. Can increase later via config. Start here.

2. **Should conversation history be in `--append-system-prompt` or `additionalInstructions`?**
   - What we know: Both end up in the system prompt. `additionalInstructions` is appended after `buildSystemPrompt()`.
   - What's unclear: Any behavioral difference in Claude's attention to each?
   - Recommendation: Include in `buildSystemPrompt()` by passing chatId. This keeps all context construction in one place. `additionalInstructions` is for per-message dynamic content (chatId label, heartbeat notes, orchestration).

3. **Exact relative time labels**
   - What we know: Phase spec says "today/yesterday/older"
   - Recommendation: Use "just now" (< 30 min), "earlier today", "yesterday", "X days ago" for more granularity.

## Sources

### Primary (HIGH confidence)

- Existing codebase (`src/memory/conversations.ts`, `src/memory/context.ts`, `src/daemon/gateway.ts`) - Read and analyzed fully
- Existing Phase 7.2 research (`.planning/phases/07.2-conversation-continuity/07.2-RESEARCH.md`) - Prior architecture decisions
- [Token counting - Claude API Docs](https://platform.claude.com/docs/en/build-with-claude/token-counting) - Official token counting reference

### Secondary (MEDIUM confidence)

- [Counting Claude Tokens Without a Tokenizer](https://blog.gopenai.com/counting-claude-tokens-without-a-tokenizer-e767f2b6e632) - 4:1 character ratio validation
- [Context Window Management Strategies](https://www.getmaxim.ai/articles/context-window-management-strategies-for-long-context-ai-agents-and-chatbots/) - Tiered context patterns
- [LLM Chat History Summarization Guide](https://mem0.ai/blog/llm-chat-history-summarization-guide-2025) - Summary buffer pattern

### Tertiary (LOW confidence)

- [Context Engineering Guide](https://www.flowhunt.io/blog/context-engineering/) - General context engineering principles

## Metadata

**Confidence breakdown:**

- Standard stack: HIGH -- No new dependencies; all existing
- Architecture: HIGH -- Builds directly on existing codebase patterns (XML tags, conversations table, system prompt building)
- Thread detection: MEDIUM -- Time-proximity heuristic is simple but may need tuning
- Token estimation: HIGH -- 4:1 ratio is industry standard for budget enforcement
- Pitfalls: HIGH -- Identified through codebase analysis, not speculation

**Research date:** 2026-02-06
**Valid until:** 2026-04-06 (stable codebase, no external dependency changes)
