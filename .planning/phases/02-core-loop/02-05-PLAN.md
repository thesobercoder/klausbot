---
phase: 02-core-loop
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - src/memory/embeddings.ts
  - src/memory/search.ts
  - src/memory/logger.ts
  - src/memory/context.ts
  - src/memory/index.ts
  - src/daemon/gateway.ts
  - package.json
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Semantic search returns relevant memories when queried about past topics"
    - "User can query past conversations using natural language (not just keywords)"
    - "System degrades to grep when OPENAI_API_KEY missing (no crashes)"
  artifacts:
    - path: "src/memory/embeddings.ts"
      provides: "Embedding generation and storage"
      exports: ["generateEmbedding", "storeEmbedding", "initializeEmbeddings"]
    - path: "src/memory/search.ts"
      provides: "Semantic search over stored embeddings"
      exports: ["semanticSearch"]
  key_links:
    - from: "src/memory/logger.ts"
      to: "src/memory/embeddings.ts"
      via: "storeEmbedding call after logging"
      pattern: "storeEmbedding"
    - from: "src/memory/context.ts"
      to: "retrieval instructions"
      via: "semantic search instructions added"
      pattern: "semantic.*search"
---

<objective>
Close MEM-05 gap: Add vector embedding infrastructure for semantic memory retrieval.

Purpose: Enable Claude to find relevant past conversations using semantic similarity, not just keyword matching. User query "what did we discuss about my family?" should find conversations mentioning relatives, parents, siblings - even if "family" was never used.

Output:

- embeddings.ts: Generate + store OpenAI text-embedding-3-small vectors
- search.ts: Cosine similarity search over stored embeddings
- Modified logger.ts: Auto-embed after logging
- Modified context.ts: Instructions for semantic search
  </objective>

<execution_context>
@/home/soham/.claude/get-shit-done/workflows/execute-plan.md
@/home/soham/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-core-loop/02-VERIFICATION.md
@src/memory/logger.ts
@src/memory/context.ts
@src/memory/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add embedding infrastructure</name>
  <files>src/memory/embeddings.ts, package.json</files>
  <action>
Create embeddings.ts with:

1. **Dependencies:** Add `openai` package to package.json (latest v4.x)

2. **Embedding generation:**
   - Use OpenAI text-embedding-3-small (1536 dimensions, $0.00002/1K tokens)
   - Function: `generateEmbedding(text: string): Promise<number[]>`
   - Use OPENAI_API_KEY from env (lazy initialization like config pattern)

3. **Embedding storage:**
   - Store in ~/.klausbot/embeddings.json (simple JSON file)
   - Format: `{ entries: [{ id, text, embedding, timestamp, source }] }`
   - Function: `storeEmbedding(text: string, source: string): Promise<void>`
   - Chunking: Split long texts at ~500 chars for better retrieval

4. **Initialization:**
   - Function: `initializeEmbeddings(): void` - creates file if missing
   - Handle missing OPENAI_API_KEY gracefully (log warning, skip embedding)

5. **Error handling:**
   - Rate limit: Log and skip (don't fail the message flow)
   - API error: Log and skip (degraded mode - grep still works)

DO NOT block the main message flow on embedding failures. This is an enhancement, not critical path.
</action>
<verify> - npm install succeeds - npm run build succeeds - embeddings.ts exports generateEmbedding, storeEmbedding, initializeEmbeddings
</verify>
<done>
Embedding module exists with OpenAI integration, graceful degradation on errors
</done>
</task>

<task type="auto">
  <name>Task 2: Create semantic search module</name>
  <files>src/memory/search.ts</files>
  <action>
Create search.ts with:

1. **Cosine similarity function:**
   - Pure TypeScript (no dependencies)
   - Function: `cosineSimilarity(a: number[], b: number[]): number`

2. **Semantic search function:**
   - Function: `semanticSearch(query: string, topK?: number): Promise<SearchResult[]>`
   - SearchResult: `{ text: string, score: number, source: string, timestamp: string }`
   - Default topK = 5
   - Load embeddings.json, generate query embedding, find top K by similarity
   - Filter by minimum score threshold (0.7)

3. **Search output format:**
   - Return markdown-friendly format for Claude to consume
   - Include source reference (which conversation file)
   - Sort by relevance score descending

4. **Handle edge cases:**
   - No embeddings file: return empty array
   - No OPENAI_API_KEY: return empty array with warning
   - All scores below threshold: return empty array
     </action>
     <verify>
   - npm run build succeeds
   - search.ts exports semanticSearch
   - Verify cosineSimilarity works:
     `bash
node -e "const cs = (a,b) => { let d=0,na=0,nb=0; for(let i=0;i<a.length;i++){d+=a[i]*b[i];na+=a[i]*a[i];nb+=b[i]*b[i]} return d/Math.sqrt(na*nb) }; console.log(cs([1,2,3],[1,2,3]))"
`
     Must output 1 (or 0.9999...)
     </verify>
     <done>
     Semantic search module returns relevant results sorted by score
     </done>
     </task>

<task type="auto">
  <name>Task 3: Wire embedding to logger and update instructions</name>
  <files>src/memory/logger.ts, src/memory/context.ts, src/memory/index.ts, src/daemon/gateway.ts</files>
  <action>
1. **Modify logger.ts:**
   - Import storeEmbedding from ./embeddings.js
   - After logAssistantMessage writes to file (after line 82), call:
     `storeEmbedding(content, 'assistant-' + getToday()).catch(() => {})`
   - Fire-and-forget (.catch to avoid blocking)
   - Only embed assistant messages (user messages less useful for retrieval)

2. **Modify context.ts getRetrievalInstructions():**
   - Add semantic search section to memory instructions:

   ```
   ## Semantic Search

   For conceptual queries ("what did we discuss about X?"), use semantic search:
   1. Read ~/.klausbot/embeddings.json
   2. The search.ts module provides semanticSearch() for similarity matching
   3. Semantic search finds related content even without exact keywords

   Fallback to Grep if semantic search unavailable or OPENAI_API_KEY not set.
   ```

3. **Update index.ts exports:**
   - Add exports from ./embeddings.js and ./search.js
   - Export: initializeEmbeddings, storeEmbedding, semanticSearch

4. **Gateway initialization (src/daemon/gateway.ts):**
   - Import initializeEmbeddings from '../memory/index.js'
   - In startGateway(), after line 37 (after initializeIdentity(log)), add:
     `initializeEmbeddings();`
     </action>
     <verify>
   - npm run build succeeds
   - grep -n "storeEmbedding" src/memory/logger.ts shows call after logAssistantMessage
   - grep -n "initializeEmbeddings" src/daemon/gateway.ts shows initialization call
   - Context instructions mention semantic search
   - Index.ts exports all new functions
     </verify>
     <done>
     Embedding generation wired to message flow, instructions updated for Claude
     </done>
     </task>

<task type="auto">
  <name>Task 4: Runtime verification - embeddings populate on message</name>
  <files>~/.klausbot/embeddings.json</files>
  <action>
Runtime test to verify full wiring works end-to-end:

1. **Setup:** Ensure OPENAI_API_KEY is set in environment

2. **Send test message:** Use existing test infrastructure or manual Telegram message

3. **Verify embeddings.json populated:**

   ```bash
   # Check file exists and has entries
   cat ~/.klausbot/embeddings.json | head -20
   ```

4. **Verify graceful degradation:**
   ```bash
   # Unset API key and verify no crash
   OPENAI_API_KEY="" node -e "import('./dist/memory/embeddings.js').then(m => m.storeEmbedding('test', 'test')).catch(e => console.log('Graceful:', e.message))"
   ```
   Should log warning, not crash.
   </action>
   <verify> - ~/.klausbot/embeddings.json exists after assistant response - File contains at least one entry with embedding array - Missing API key logs warning but doesn't crash
   </verify>
   <done>
   End-to-end wiring confirmed: messages trigger embeddings storage
   </done>
   </task>

</tasks>

<verification>
After all tasks:

1. **Build verification:**

   ```bash
   npm run build
   ```

2. **Dependency check:**

   ```bash
   npm ls openai
   ```

3. **Export verification:**

   ```bash
   grep -r "export.*semanticSearch\|storeEmbedding\|initializeEmbeddings" src/memory/
   ```

4. **Wiring verification:**

   ```bash
   grep -n "storeEmbedding" src/memory/logger.ts
   grep -n "initializeEmbeddings" src/daemon/gateway.ts
   grep -n "semantic" src/memory/context.ts
   ```

5. **Graceful degradation verification:**

   ```bash
   OPENAI_API_KEY="" node -e "import('./dist/memory/embeddings.js').then(m => m.storeEmbedding('test', 'test').then(() => console.log('OK'))).catch(e => console.log('Graceful fail:', e.message))"
   ```

   Must not crash; should log warning or graceful message.

6. **Runtime verification (with API key):**
   After sending a test message, verify:
   ```bash
   ls -la ~/.klausbot/embeddings.json
   cat ~/.klausbot/embeddings.json | head -5
   ```
   File should exist and contain entries array.
   </verification>

<success_criteria>

1. Build succeeds with new openai dependency
2. embeddings.ts generates and stores embeddings via OpenAI API
3. search.ts performs cosine similarity search over stored embeddings
4. logger.ts auto-embeds assistant responses (fire-and-forget)
5. context.ts includes semantic search in Claude's retrieval instructions
6. Missing OPENAI_API_KEY degrades gracefully (grep fallback)
   </success_criteria>

<output>
After completion, create `.planning/phases/02-core-loop/02-05-SUMMARY.md`
</output>
