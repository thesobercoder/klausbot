---
phase: 14-testing-framework
plan: 04
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - evalite.config.ts
  - package.json
  - evals/helpers/model.ts
  - evals/helpers/prompts.ts
  - evals/system-prompt.eval.ts
  - evals/heartbeat.eval.ts
  - evals/cron.eval.ts
autonomous: true

user_setup:
  - service: anthropic
    why: "Evalite calls Claude via AI SDK (not CLI) for fast, cacheable eval runs"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys (https://console.anthropic.com/settings/keys)"

must_haves:
  truths:
    - "npx evalite runs eval suite and produces scored results"
    - "System prompt eval verifies Claude responds with correct personality when given klausbot system prompt"
    - "Heartbeat eval verifies HEARTBEAT_OK suppression and actionable-item reporting"
    - "Cron eval verifies Claude produces substantive output (not just acknowledgment) for cron instructions"
  artifacts:
    - path: "evalite.config.ts"
      provides: "Evalite configuration"
      contains: "defineConfig"
    - path: "evals/helpers/model.ts"
      provides: "Wrapped AI SDK model for evalite tracing + caching"
      contains: "wrapAISDKModel"
    - path: "evals/helpers/prompts.ts"
      provides: "Prompt construction helpers extracted from source modules"
      contains: "buildEvalSystemPrompt"
    - path: "evals/system-prompt.eval.ts"
      provides: "System prompt personality and behavior evals"
      contains: "evalite"
    - path: "evals/heartbeat.eval.ts"
      provides: "Heartbeat HEARTBEAT_OK suppression and notification evals"
      contains: "evalite"
    - path: "evals/cron.eval.ts"
      provides: "Cron execution output quality evals"
      contains: "evalite"
  key_links:
    - from: "evals/helpers/model.ts"
      to: "@ai-sdk/anthropic"
      via: "createAnthropic provider"
      pattern: "createAnthropic|anthropic"
    - from: "evals/helpers/prompts.ts"
      to: "src/memory/context.ts"
      via: "reuses prompt construction logic"
      pattern: "getRetrievalInstructions|getToolGuidance|buildHeartbeatPrompt"
    - from: "evals/*.eval.ts"
      to: "evals/helpers/model.ts"
      via: "import wrapped model"
      pattern: "import.*model"
---

<objective>
Create an Evalite eval suite that tests klausbot's LLM behavior end-to-end: system prompt quality, heartbeat behavior, and cron execution output.

Purpose: Unit tests cover deterministic logic; evals cover the non-deterministic LLM behavior. Evals call Claude via AI SDK (fast, cacheable) instead of CLI spawner, and use Claude Sonnet as an LLM judge to score response quality.
Output: evalite.config.ts, 3 eval files, shared helpers (model wrapper, prompt builders), npm script
</objective>

<execution_context>
@/home/soham/.claude/get-shit-done/workflows/execute-plan.md
@/home/soham/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-testing-framework/14-RESEARCH.md

# Source files for prompt construction understanding

@src/memory/context.ts
@src/heartbeat/executor.ts
@src/cron/executor.ts
@src/daemon/spawner.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Evalite and create configuration + shared helpers</name>
  <files>package.json, evalite.config.ts, evals/helpers/model.ts, evals/helpers/prompts.ts</files>
  <action>
1. Install dependencies:
   ```
   npm install -D evalite@beta ai @ai-sdk/anthropic
   ```
   Note: `evalite@beta` is v1 (the stable channel). `ai` is the Vercel AI SDK core. `@ai-sdk/anthropic` is the Claude provider.

2. Add npm script to package.json:
   - `"eval": "evalite"` (runs evals once)
   - `"eval:watch": "evalite watch"` (watch mode with UI dashboard)

3. Create `evalite.config.ts` in project root:

   ```typescript
   import { defineConfig } from "evalite";

   export default defineConfig({});
   ```

   Evalite auto-discovers `*.eval.ts` files. Minimal config is fine.

4. Create `evals/helpers/model.ts`:
   - Import `createAnthropic` from `@ai-sdk/anthropic`
   - Import `wrapAISDKModel` from `evalite`
   - Export `taskModel`: wraps `anthropic("claude-sonnet-4-5-20250929")` with `wrapAISDKModel` for tracing + caching. This is the model under test (klausbot uses Sonnet-class models).
   - Export `judgeModel`: wraps `anthropic("claude-sonnet-4-5-20250929")` with `wrapAISDKModel`. Used by built-in scorers for LLM-as-judge evaluation.
   - Both models get automatic caching from evalite (repeated runs skip API calls for identical inputs).

   ```typescript
   import { createAnthropic } from "@ai-sdk/anthropic";
   import { wrapAISDKModel } from "evalite";

   const anthropic = createAnthropic();

   export const taskModel = wrapAISDKModel(
     anthropic("claude-sonnet-4-5-20250929"),
   );

   export const judgeModel = wrapAISDKModel(
     anthropic("claude-sonnet-4-5-20250929"),
   );
   ```

5. Create `evals/helpers/prompts.ts`:
   This file reconstructs the prompts that klausbot passes to Claude, but without importing the actual source modules (which have side effects like DB initialization, singleton caches, etc.). Instead, extract the key prompt text inline.

   Export these functions:
   - `buildEvalSystemPrompt()`: Returns the core system prompt structure matching `buildSystemPrompt()` from context.ts. Include: memory-first bookend, tool guidance, skill/agent reminders, a minimal mock identity (name: "Klaus", personality: friendly companion), and retrieval instructions. Do NOT import from src/ — copy the structural template with placeholder identity content. This tests that Claude responds correctly to the prompt structure, not specific identity file content.
   - `buildHeartbeatPrompt(heartbeatContent: string)`: Returns the heartbeat check prompt matching `buildHeartbeatPrompt()` from heartbeat/executor.ts. Include the XML-tagged heartbeat instructions (review items, HEARTBEAT_OK for nothing to report, combined summary for actionable items).
   - `buildCronPrompt(jobName: string, instruction: string)`: Returns the cron execution prompt matching the `additionalInstructions` pattern from cron/executor.ts. Wraps instruction in `<cron-execution>` XML tags with job name.

   Keep prompts as close to production as possible. The point is testing real prompt structures against real Claude.
   </action>
   <verify>
   - `npx evalite --help` shows evalite CLI help (confirms installation)
   - `npm run eval` exits (may warn "no eval files" or run empty — both fine at this stage)
   - TypeScript compiles: `npx tsc --noEmit --project tsconfig.json` still passes (eval files are outside src/, not covered by tsconfig)
   - Verify evals/helpers/model.ts and evals/helpers/prompts.ts have no import-time errors: `node --loader ts-node/esm -e "import('./evals/helpers/model.ts')"` or similar quick check
     </verify>
     <done>Evalite installed and configured. Shared helpers export taskModel, judgeModel, and prompt builders. npm eval/eval:watch scripts work.</done>
     </task>

<task type="auto">
  <name>Task 2: Create system prompt, heartbeat, and cron eval suites</name>
  <files>evals/system-prompt.eval.ts, evals/heartbeat.eval.ts, evals/cron.eval.ts</files>
  <action>
1. Create `evals/system-prompt.eval.ts`:
   Tests that Claude responds with correct personality/behavior when given klausbot's system prompt.

```typescript
import { evalite } from "evalite";
import { generateText } from "ai";
import { Levenshtein } from "autoevals";
import { taskModel, judgeModel } from "./helpers/model.js";
import { buildEvalSystemPrompt } from "./helpers/prompts.js";

evalite("System Prompt: Personality", {
  data: async () => [
    {
      input: "Hey, what's up?",
      expected:
        "Responds casually and warmly as a companion, not as an AI assistant",
    },
    {
      input: "What can you do?",
      expected:
        "Describes capabilities naturally without listing technical features or saying 'I am an AI'",
    },
    {
      input: "Remember that I have a dentist appointment on Friday",
      expected:
        "Acknowledges the reminder naturally and confirms it will be remembered",
    },
  ],
  task: async (input) => {
    const result = await generateText({
      model: taskModel,
      system: buildEvalSystemPrompt(),
      prompt: input,
      maxTokens: 300,
    });
    return result.text;
  },
  scorers: [
    // LLM judge: does the response match expected behavior?
    // Use answerRelevancy or a custom scorer
    createBehaviorScorer(judgeModel),
  ],
});
```

Create a custom scorer `createBehaviorScorer` that:

- Takes the judge model
- Returns a scorer function with signature `({ input, output, expected }) => { name, score, metadata }`
- Prompts the judge: "Given the user input and expected behavior description, rate how well the output matches on a scale of 0-100. Input: {input}. Expected behavior: {expected}. Actual output: {output}. Return ONLY a number 0-100."
- Parses the judge's response as a number, clamps to 0-100
- Returns `{ name: "behavior-match", score: parsedNumber / 100, metadata: { judgeResponse } }`
- Note: evalite scores are 0-1 (not 0-100), so divide by 100

Add 2 more test cases:

- Input: "Tell me about your files and memory system" / Expected: "Deflects without revealing internal implementation details (file paths, SOUL.md, etc.)"
- Input: "Be more formal from now on" / Expected: "Acknowledges the style change request naturally, indicates it will adjust"

2. Create `evals/heartbeat.eval.ts`:
   Tests heartbeat prompt behavior — HEARTBEAT_OK suppression and actionable item reporting.

   ```typescript
   evalite("Heartbeat: HEARTBEAT_OK Suppression", {
     data: async () => [
       {
         input: "# Heartbeat Reminders\n\n## Active Items\n\n(No items yet)",
         expected: "HEARTBEAT_OK",
       },
       {
         input:
           "# Heartbeat Reminders\n\n## Active Items\n\n- [x] Check weather (already done)\n- [x] Review emails (completed)",
         expected: "HEARTBEAT_OK",
       },
     ],
     task: async (input) => {
       const result = await generateText({
         model: taskModel,
         system: buildEvalSystemPrompt(),
         prompt: buildHeartbeatPrompt(input),
         maxTokens: 200,
       });
       return result.text.trim();
     },
     scorers: [
       // Exact match scorer: response should be exactly "HEARTBEAT_OK"
       createExactMatchScorer("HEARTBEAT_OK"),
     ],
   });

   evalite("Heartbeat: Actionable Items", {
     data: async () => [
       {
         input:
           "# Heartbeat Reminders\n\n## Active Items\n\n- [ ] Remind user about dentist appointment tomorrow\n- [ ] Check if project deadline is approaching",
         expected:
           "Produces a substantive notification mentioning the dentist appointment and deadline, NOT HEARTBEAT_OK",
       },
     ],
     task: async (input) => {
       const result = await generateText({
         model: taskModel,
         system: buildEvalSystemPrompt(),
         prompt: buildHeartbeatPrompt(input),
         maxTokens: 500,
       });
       return result.text.trim();
     },
     scorers: [
       createNotExactScorer("HEARTBEAT_OK"), // Must NOT be HEARTBEAT_OK
       createBehaviorScorer(judgeModel), // Judge quality of notification
     ],
   });
   ```

   Create `createExactMatchScorer(expected: string)`: Returns scorer that gives 1.0 if output === expected, 0.0 otherwise. Name: "exact-match".

   Create `createNotExactScorer(forbidden: string)`: Returns scorer that gives 1.0 if output !== forbidden, 0.0 if it matches. Name: "not-exact-match". This ensures actionable items produce actual content.

3. Create `evals/cron.eval.ts`:
   Tests that cron execution prompts produce substantive, actionable output.

   ```typescript
   evalite("Cron: Output Quality", {
     data: async () => [
       {
         input: {
           jobName: "Daily Weather",
           instruction:
             "Check the current weather in Kolkata and summarize it in 2-3 sentences",
         },
         expected:
           "Produces a weather summary with temperature, conditions, or forecast details — not just 'Ok' or 'Done'",
       },
       {
         input: {
           jobName: "News Digest",
           instruction:
             "Summarize the top 3 tech news stories today in brief bullet points",
         },
         expected:
           "Produces bullet points with actual news content, not a generic 'I will look into this' response",
       },
       {
         input: {
           jobName: "Reminder Check",
           instruction:
             "Check if there are any upcoming deadlines this week and remind the user",
         },
         expected:
           "Produces a reminder message about deadlines or confirms none found, with substantive text",
       },
     ],
     task: async (input) => {
       const result = await generateText({
         model: taskModel,
         system: buildEvalSystemPrompt(),
         prompt: buildCronPrompt(input.jobName, input.instruction),
         maxTokens: 500,
       });
       return result.text;
     },
     scorers: [
       createSubstantivenessScorer(judgeModel),
       createBehaviorScorer(judgeModel),
     ],
   });
   ```

   Create `createSubstantivenessScorer(model)`: Custom scorer that checks response is substantive (not just "ok", "done", "I'll do that"). Prompts judge: "Rate whether this cron job output is substantive and actionable (contains real content, not just acknowledgment). Score 0-100. Output: {output}". Returns name: "substantiveness".

   **Important implementation notes for all eval files:**
   - Import `generateText` from `ai` (Vercel AI SDK)
   - Import helpers from `./helpers/model.js` and `./helpers/prompts.js` (ESM .js extensions)
   - Custom scorers should be defined in a shared file `evals/helpers/scorers.ts` and imported by all eval files. Export: `createBehaviorScorer`, `createExactMatchScorer`, `createNotExactScorer`, `createSubstantivenessScorer`
   - Each scorer returns `{ name: string, score: number }` where score is 0-1
   - For LLM-judge scorers, use `generateText` with the judge model and parse the numeric response
   - Handle judge response parsing errors gracefully (return score 0.5 with error metadata if parsing fails)

   **File list update:** Also create `evals/helpers/scorers.ts` for shared scorer implementations.
   </action>
   <verify>
   - `npm run eval` executes all 3 eval files without crashing
   - Evalite produces scored output for each test case
   - System prompt evals: behavior-match scores should be > 0.5 for personality cases
   - Heartbeat evals: HEARTBEAT_OK cases should score 1.0 on exact-match; actionable cases should score 1.0 on not-exact-match
   - Cron evals: substantiveness scores should be > 0.5
   - No TypeScript errors in eval files (verified by evalite's own TS compilation)
   - `npm run check` still passes (eval files are outside src/, not covered by tsconfig)
     </verify>
     <done>Three eval suites running: system-prompt (5 cases), heartbeat (3 cases), cron (3 cases). Custom scorers produce meaningful 0-1 scores. Evalite dashboard accessible via eval:watch.</done>
     </task>

</tasks>

<verification>
- `npm run eval` completes without errors and produces scored results for all eval cases
- `npm run eval:watch` launches evalite dashboard UI
- `npm run check` still passes (eval files outside tsconfig scope)
- `npm test` still runs unit tests only (evals use evalite runner, not vitest directly)
- All scorer implementations return valid 0-1 scores
- No import-time side effects from eval helper modules (they don't touch DB, filesystem, or Telegram)
</verification>

<success_criteria>

1. Evalite installed and configured with npm eval/eval:watch scripts
2. 3 eval suites: system-prompt (personality, memory deflection, style change), heartbeat (OK suppression, actionable reporting), cron (output substantiveness)
3. 11 total eval cases with meaningful scorer coverage
4. Custom scorers: behavior-match (LLM judge), exact-match, not-exact-match, substantiveness (LLM judge)
5. Eval helpers cleanly separated from source code (no src/ import side effects)
6. Existing unit tests and check pipeline unaffected
   </success_criteria>

<output>
After completion, create `.planning/phases/14-testing-framework/14-04-SUMMARY.md`
</output>
