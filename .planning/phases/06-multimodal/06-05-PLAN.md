---
phase: 06-multimodal
plan: 05
type: execute
wave: 4
depends_on: ["06-04"]
files_modified: []
autonomous: false

must_haves:
  truths:
    - "Voice message transcribed and responded to"
    - "Image analyzed and responded to"
    - "Errors surfaced clearly when media processing fails"
  artifacts: []
  key_links: []
---

<objective>
End-to-end verification of multimodal message handling.

Purpose: Confirm voice transcription and image analysis work in production.
Output: Human-verified multimodal functionality.
</objective>

<execution_context>
@/home/soham/.claude/get-shit-done/workflows/execute-plan.md
@/home/soham/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-multimodal/06-CONTEXT.md
@.planning/phases/06-multimodal/06-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build and start gateway</name>
  <files></files>
  <action>
Build the project:
```bash
npm run build
```

Verify no build errors.

Start the gateway (user will do this in their terminal):

```bash
node dist/index.js daemon
```

Check startup logs for:

1. "Media capabilities" log with voiceTranscription and imageAnalysis status
2. No startup errors
   </action>
   <verify>
3. `npm run build` succeeds
4. Gateway starts without errors
5. Media capabilities logged
   </verify>
   <done>

- Project builds successfully
- Gateway starts with media capability logging
  </done>
  </task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Multimodal message handling:
- Voice messages transcribed via Whisper API
- Images analyzed via Claude vision
- Media errors surfaced with actionable messages
  </what-built>
  <how-to-verify>
**Prerequisites:**
- Gateway running (`node dist/index.js daemon`)
- OPENAI_API_KEY set (for voice transcription)
- Telegram bot paired

**Test 1: Voice Message**

1. Open Telegram chat with the bot
2. Record and send a voice message (e.g., say "What time is it?")
3. Wait for response
4. Expected: Bot responds to the content of your voice message
5. Check logs: Should show "Transcribed voice message" with transcript length

**Test 2: Image Message**

1. Send a photo to the bot (any image)
2. Wait for response
3. Expected: Bot describes or comments on the image content
4. Check logs: Should show "Saved image" with path
5. Check ~/.klausbot/images/: Should contain saved image in dated folder

**Test 3: Image with Caption**

1. Send a photo with a caption/question (e.g., photo of a document + "What does this say?")
2. Wait for response
3. Expected: Bot responds to both the image and the text question

**Test 4: Error Handling (if OPENAI_API_KEY missing)**

1. Stop gateway, unset OPENAI_API_KEY, restart gateway
2. Check startup logs: voiceTranscription should be false
3. Send a voice message
4. Expected: Bot responds with "Voice transcription not available" message
5. Re-set OPENAI_API_KEY and restart for subsequent tests

**Test 5: Unsupported Media Type**

1. Send a sticker or video to the bot
2. Expected: Bot responds with "I can process text, voice messages, and photos..."
   </how-to-verify>
   <resume-signal>Type "approved" if all tests pass, or describe any issues</resume-signal>
   </task>

</tasks>

<verification>
Human verification of:
1. Voice transcription working
2. Image analysis working
3. Error handling clear
4. Unsupported types handled gracefully
</verification>

<success_criteria>

- Voice messages transcribed and responded to correctly
- Images saved and analyzed by Claude
- Media errors surfaced with actionable messages
- Unsupported message types get helpful error
- All tests pass human verification
  </success_criteria>

<output>
After completion, create `.planning/phases/06-multimodal/06-05-SUMMARY.md`
</output>
